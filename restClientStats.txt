To diagnose this, you can enable a background task to log the Apache HttpClient 5 connection pool metrics. This will tell you exactly how many requests are "Leasing" (active) versus "Pending" (queued).
1. Monitoring the Connection Pool
Add this temporary "Monitor" component to your Spring Boot application. It will print the pool status every second while you run your k6 test.
@Component
public class ConnectionPoolMonitor {

    private final PoolingHttpClientConnectionManager connectionManager;

    // Inject the manager you used in your RestClient config
    public ConnectionPoolMonitor(PoolingHttpClientConnectionManager connectionManager) {
        this.connectionManager = connectionManager;
    }

    @Scheduled(fixedDelay = 1000)
    public void logPoolStats() {
        PoolStats totalStats = connectionManager.getTotalStats();
        System.out.printf("[POOL STATS] Max: %d | Leased: %d | Pending: %d | Available: %d%n",
                totalStats.getMax(),
                totalStats.getLeased(),
                totalStats.getPending(), // This is the "waiting room"
                totalStats.getAvailable());
    }
}

What to look for during the 15s delays:
 * If Pending is greater than 0, your threads are stuck in a queue.
 * If Leased is stuck at its maximum (e.g., 5 or 25), your pool is too small.
2. Tuning the Timeouts
Since you are seeing "Read Timeouts" at the 15s mark, you should explicitly define your timeout strategy. In HttpClient 5, these are configured via RequestConfig and ConnectionConfig.
@Bean
public RestClient restClient() {
    // 1. Connection-level settings
    ConnectionConfig connectionConfig = ConnectionConfig.custom()
            .setConnectTimeout(Timeout.ofSeconds(2)) // Time to establish TCP handshake
            .setSocketTimeout(Timeout.ofSeconds(15)) // Time spent waiting for data (Read Timeout)
            .build();

    // 2. Request-level settings (The "Queue" timeout)
    RequestConfig requestConfig = RequestConfig.custom()
            .setConnectionRequestTimeout(Timeout.ofSeconds(5)) // How long to wait in the pool queue
            .build();

    PoolingHttpClientConnectionManager connectionManager = PoolingHttpClientConnectionManagerBuilder.create()
            .setDefaultConnectionConfig(connectionConfig)
            .setMaxTotal(200)
            .setDefaultMaxPerRoute(100)
            .build();

    CloseableHttpClient httpClient = HttpClients.custom()
            .setConnectionManager(connectionManager)
            .setDefaultRequestConfig(requestConfig)
            .build();

    return RestClient.builder()
            .requestFactory(new HttpComponentsClientHttpRequestFactory(httpClient))
            .build();
}

3. Why k6 "Direct" is different
When you run k6 directly against the Internal API:
 * No Queueing: k6 spins up OS-level threads/goroutines that open connections immediately.
 * Ephemeral Ports: k6 uses a wide range of source ports to avoid the bottlenecks a single JVM instance faces.
 * No Intermediate Logic: Your Spring Boot service likely has a Tomcat Thread Pool that might also be saturating.
Final Verification Step
Run your k6 test with the logPoolStats monitor active. If you see Pending: 50 and Leased: 5, you have confirmed that the 15s delay is simply requests waiting for their turn to use one of the 5 "pipes" to the internal API.
Would you like me to help you adjust your Tomcat thread settings as well, in case the bottleneck is at the entry point of your service?





This discrepancy (600ms vs. 15s) is the "smoking gun" for Connection Pool Starvation.
When you hit the API directly with k6, there is no middleman. When you go through your Spring Boot service, your request is likely spending 14.4 seconds sitting in a queue inside your own service before it even attempts to talk to the internal API.
Here is exactly why that 15-second gap is happening:
1. The "Waiting Room" Effect (Connection Request Timeout)
In Apache HttpClient 5 (the engine under RestClient), there are three distinct timeouts. The one killing you right now is the Connection Request Timeout.
 * Direct k6: Request → Internal API (600ms) → Done.
 * Through Service: 1.  Request enters your Service.
   2.  RestClient asks the Pool for a connection.
   3.  The Pool is full (e.g., all 5 default connections are busy).
   4.  Your request waits in a queue for a connection to become free.
   5.  It waits and waits... eventually, a connection opens up after 14 seconds, or it hits a limit and throws a Read Timeout because the total time elapsed exceeded your configured threshold.
2. Why "Read Timeout" specifically?
You might wonder: "If it's waiting for a connection, why do I get a Read Timeout?"
Most developers set a readTimeout (e.g., 15s) on the RestClient. Many HTTP clients start the "total request clock" the moment the execution begins. If the request spends 14.5 seconds waiting for a connection and 0.6 seconds actually talking to the API, the total time is 15.1 seconds.
The client sees 15.1 > 15.0 and kills the request with a Read Timeout, even though the internal API was fast once it finally got the chance to talk.
3. The Math of the Bottleneck
Let's look at the numbers:
 * Internal API Latency: 600ms (0.6s)
 * Default Connections per Route: 5
 * Max Throughput of your Service: 5 \text{ connections} / 0.6s \approx 8.3 \text{ requests per second}
If k6 is sending 50 requests per second, but your service can only physically process 8.3 requests per second due to the pool limit, the "backlog" grows instantly.
| Request # | Wait Time in Queue | Total Time Seen by k6 |
|---|---|---|
| 1-5 | 0s | 600ms |
| 6-10 | 600ms | 1.2s |
| 45-50 | ~5.4s | 6.0s |
| 100+ | > 10s | Timeout! |
4. The Fix: Open the Floodgates
To match the performance you see when running k6 directly, you must increase the DefaultMaxPerRoute.
In your RestClient configuration:
PoolingHttpClientConnectionManager connectionManager = PoolingHttpClientConnectionManagerBuilder.create()
    .setMaxTotal(500)          // Allow up to 500 total connections
    .setDefaultMaxPerRoute(200) // Allow 200 concurrent connections to the SAME internal API
    .build();

CloseableHttpClient httpClient = HttpClients.custom()
    .setConnectionManager(connectionManager)
    .build();

RestClient restClient = RestClient.builder()
    .requestFactory(new HttpComponentsClientHttpRequestFactory(httpClient))
    .build();

5. Verify with this Test
To prove this is the issue, run your k6 test against your service with only 1 Virtual User (VU).
 * If the response time is 600ms, it’s 100% a pooling/concurrency issue.
 * If the response time is still 15s, you have a network routing/proxy issue (like a misconfigured DNS or a slow load balancer between the two services).
Would you like me to show you how to log the Pool Statistics in your Spring Boot logs so you can see the "Leased" vs "Pending" connections in real-time?






======//////


That is a classic "Performance Paradox." Your settings look healthy on paper, and 10 VUs (Virtual Users) should easily be handled by a pool of 50.
If your internal API only takes 600ms, but your service hits a 15s Read Timeout with only 10 concurrent users, the problem isn't the size of the pool—it’s that the connections in the pool are becoming "Stuck" or "Zombies."
Here are the most likely reasons why this is happening despite your configuration:
1. The "Connection Leak" (Most Likely)
If your code calling RestClient does not fully consume the response body (e.g., you check a status code but don't read the string/object), that connection is not returned to the pool.
 * The Symptom: Your 10 VUs quickly "leak" 50 connections. Once the 50th connection is leaked, the 51st request waits for the connection-request-timeout.
 * The Math: 50 connections / 10 VUs = Each VU only needs to leak 5 times before the pool is dry.
 * Why the 15s? Your request waits for a connection, finally gets one (perhaps after an idle eviction), but because the system is so backed up, the total time from start to finish crosses the 15s Socket Timeout.
2. DNS Resolution or "IPv6 Ghosting"
Sometimes the HttpClient spends a massive amount of time resolving the internal API's hostname.
 * If your internal API is at internal-api.local, the client might try to resolve it via IPv6 first, wait for a 5s timeout, then failover to IPv4.
 * If this happens on every new connection, it eats into your total time budget.
3. The "Slow Consumer" on the Internal API
If the internal API is fast for k6 (direct) but slow for your service, check if there is a Proxy or Sidecar (like Istio/Envoy) between your service and the internal API.
 * A direct k6 call might bypass a proxy that your Spring Boot service is forced to use.
 * If that proxy is struggling, it will hold the TCP socket open but send no data, eventually triggering your 15s Read Timeout.
How to Diagnose This Immediately
Step 1: Enable Connection Pool Logging
Add this to your application.properties to see exactly what the pool is doing:
logging.level.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager=DEBUG
logging.level.org.apache.hc.client5.http.wire=DEBUG

Look for: Connection request: [route: ...][total kept alive: X; route allocated: Y of 50; total allocated: Z of 200]. If "allocated" hits 50 and stays there, you have a leak.
Step 2: Ensure Response Consumption
Ensure you are using RestClient in a way that closes the stream. For example:
// GOOD: .body() or .toEntity() automatically handles closing
var response = restClient.get()
    .uri("/data")
    .retrieve()
    .body(String.class); 

// RISKY: If you use .exchange(), you MUST close the response manually
restClient.get()
    .uri("/data")
    .exchange((request, response) -> {
        // If you don't read response.getBody(), the connection leaks!
        return response.getStatusCode();
    });

Step 3: Check the "Time to First Byte" (TTFB)
In your k6 results, look at the http_req_waiting metric.
 * If http_req_waiting is near 15s, the internal API is simply not responding to your service.
 * If http_req_connecting or http_req_blocked is high, the issue is inside your Spring Boot pool.
Would you like me to show you a specific RestTemplate or RestClient interceptor that logs how long the connection pool wait time is for every request?




=========≈==============================




To track why your 600ms API is suddenly taking 15 seconds, we need to distinguish between "Time spent waiting for a connection" and "Time spent waiting for the server to talk."
Standard interceptors usually only measure the time after the connection is grabbed. To see the full picture—including the "waiting room" time—we have to look at the ClientHttpRequestFactory level or use a custom metric.
1. For RestTemplate (using Apache HC5)
With RestTemplate, we can wrap the HttpComponentsClientHttpRequestFactory to see exactly how long the pool took to give us a "lease."
public class PoolingTimerRequestFactory extends HttpComponentsClientHttpRequestFactory {

    public PoolingTimerRequestFactory(HttpClient httpClient) {
        super(httpClient);
    }

    @Override
    protected HttpContext createHttpContext(HttpMethod httpMethod, URI uri) {
        HttpContext context = HttpClientContext.create();
        context.setAttribute("request.start_time", System.currentTimeMillis());
        return context;
    }
}

// In your Interceptor:
public class PerformanceInterceptor implements ClientHttpRequestInterceptor {
    @Override
    public ClientHttpResponse intercept(HttpRequest request, byte[] body, 
                                        ClientHttpRequestExecution execution) throws IOException {
        long start = System.currentTimeMillis();
        
        ClientHttpResponse response = execution.execute(request, body);
        
        long duration = System.currentTimeMillis() - start;
        System.out.printf("Total Request Time: %dms | URI: %s%n", duration, request.getURI());
        
        return response;
    }
}

2. For RestClient (The Modern Way)
Since RestClient uses a functional approach, you can bake the timing directly into the requestInterceptor. However, to catch the 15s Read Timeout before it kills the log, use a try-finally block.
RestClient restClient = RestClient.builder()
    .requestFactory(new HttpComponentsClientHttpRequestFactory(httpClient))
    .requestInterceptor((request, body, execution) -> {
        long start = System.nanoTime();
        try {
            return execution.execute(request, body);
        } finally {
            long elapsed = (System.nanoTime() - start) / 1_000_000;
            if (elapsed > 1000) { // Log anything over 1 second
                System.err.printf("[PERF ALERT] Request to %s took %dms%n", 
                                  request.getURI(), elapsed);
            }
        }
    })
    .build();

3. The "Hidden" Metric: Pool Stats
The most effective way to see if your 50 connections are "stuck" is to expose the pool's internal state. If your Leased count hits 50 and stays there during your k6 test, you have found your leak.
Add this "Watcher" thread or a scheduled task to your service during the load test:
@Scheduled(fixedRate = 1000)
public void logPoolStats() {
    // You need a reference to your PoolingHttpClientConnectionManager
    PoolStats totalStats = connectionManager.getTotalStats();
    System.out.printf("Pool Stats -> Leased: %d, Pending: %d, Available: %d, Max: %d%n",
        totalStats.getLeased(),   // Connections currently in use
        totalStats.getPending(),  // Requests waiting for a connection (THIS IS YOUR 15s CULPRIT)
        totalStats.getAvailable(),// Free connections in the pool
        totalStats.getMax()       // Absolute ceiling
    );
}

What to look for in the logs:
 * Pending > 0: This means your 50 "Max Per Route" is too low or connections aren't being returned.
 * Leased == Max: You have a connection leak. Somewhere in your code, a response is being received but the input stream is never closed.
 * High Total Time but Low Internal API time: If logPoolStats shows 0 pending but the interceptor shows 15s, the bottleneck is the network/DNS or the internal API's TCP handshake.
Would you like me to show you the common "Connection Leak" code patterns in Spring Boot so you can check your business logic for them?



=======================================================


To force-clean the pool and prevent these "zombies" from hanging around for 15 seconds, you can configure a Background Eviction Thread.
While this is a "band-aid" for a leak, it ensures that if a connection is abandoned or the server silently drops it, your Spring Boot service won't sit there waiting for it forever.
1. The "Brute Force" Evictor Configuration
In Apache HttpClient 5, you can tell the connection manager to proactively scan for and kill idle or expired connections.
@Bean
public HttpClient hc5Client() {
    PoolingHttpClientConnectionManager connectionManager = PoolingHttpClientConnectionManagerBuilder.create()
            .setMaxTotal(200)
            .setDefaultMaxPerRoute(50)
            // 1. Validate connections that have been sitting idle before leasing them
            .setValidateAfterInactivity(TimeValue.ofSeconds(2)) 
            .build();

    return HttpClients.custom()
            .setConnectionManager(connectionManager)
            // 2. Kill connections that are idle for more than 10 seconds
            .evictIdleConnections(TimeValue.ofSeconds(10))
            // 3. Kill connections that have outlived their TTL
            .evictExpiredConnections()
            .build();
}

2. Why this stops the 15s Timeout
Without the evictor, if a connection becomes "stale" (the internal API closed it but your service didn't notice), your RestClient might try to reuse it.
 * It sends the request into a "black hole."
 * It waits for the Socket Timeout (15s) to trigger.
 * With setValidateAfterInactivity, the client checks if the socket is still "alive" before giving it to your code. If it's dead, it drops it and gets you a fresh one in milliseconds.
3. The "Silent Killer": Keep-Alive Mismatch
If your Service thinks a connection is good for 25s (your current setting), but the internal API has a keep-alive timeout of 5s, you will have a "Race Condition" window of 20 seconds where your service will try to use a dead socket.
The Fix: Always set your client-side keep-alive to be shorter than the server's.
// Force a 5-second keep-alive if the server doesn't provide one
DefaultConnectionKeepAliveStrategy keepAliveStrategy = new DefaultConnectionKeepAliveStrategy() {
    @Override
    public TimeValue getKeepAliveDuration(HttpResponse response, HttpContext context) {
        TimeValue duration = super.getKeepAliveDuration(response, context);
        if (duration.toMilliseconds() < 0) {
            // Server didn't send a Keep-Alive header, default to 5 seconds
            return TimeValue.ofSeconds(5);
        }
        return duration;
    }
};

// Add to your HttpClient builder
.setKeepAliveStrategy(keepAliveStrategy)

4. Summary of the "600ms vs 15s" Fix
 * The Leak: Check if you are using .exchange() without closing the body.
 * The Pool: Ensure MaxPerRoute is at least 50 (which you have).
 * The Evictor: Enable evictIdleConnections to clear the "zombies."
 * The Validation: Use setValidateAfterInactivity to ensure you don't lease a dead socket.
Would you like me to provide a small code snippet to log the specific "Connection Request Timeout" exception so you can confirm if the 15s is spent waiting for a connection or reading from it?

